I met a little difficulty in crawling task. The the crawler model often raise exceptions, i guess the problem is it cannot hold a page too long. I decided to crawl the urls of the news first, then open the urls individually to solve it. I'm also considering the elements besides title and content. Finally the urls was got. I also improved the content crawler to make it run in multiple threads. It can improve the efficience but make the program unstable. Then I tried to make it stable and it seems work. The next task is to crawl such a big data base.
